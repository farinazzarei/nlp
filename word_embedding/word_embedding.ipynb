{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dbd0308-5e67-4d4b-93a6-7de5cca22bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from tqdm import trange\n",
    "nltk.download('punkt') # for text tokenization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8465e841-af31-4d48-98d3-3c98133e1ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.total_words = 0\n",
    "        self.word_count = defaultdict(int)\n",
    "\n",
    "    # Crawling through a sentence and counting each word, creating an index for each word and vice versa\n",
    "    def build_vocab(self, sentence, min_count=2):\n",
    "        for word in sentence:\n",
    "            self.word_count[word] += 1\n",
    "        idx = 0\n",
    "        for word, count in self.word_count.items():\n",
    "            if count >= min_count:\n",
    "                self.word2idx.update({word: idx})\n",
    "                idx += 1\n",
    "\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.total_words = sum(\n",
    "            [count for word, count in self.word_count.items() if count >= min_count]\n",
    "        )\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        return self.word2idx.get(word, -1)\n",
    "\n",
    "    def index_to_word(self, index):\n",
    "        return self.idx2word.get(index, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1df99be1-4e3e-42c9-a240-c77fbcebb81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating (context_word,center_word) in a sentence which context_word is neighbor and center_word is center word in a window\n",
    "def generate_training_data(vocab, sentence, window_size=2):\n",
    "    training_data = []\n",
    "    sentence_indices = [\n",
    "        vocab.word_to_index(word)\n",
    "        for word in sentence\n",
    "        if vocab.word_to_index(word) != -1\n",
    "    ]\n",
    "\n",
    "    for center_idx, center_word in enumerate(sentence_indices):\n",
    "        context_start = max(0, center_idx + window_size)\n",
    "        context_end = min(len(sentence_indices), center_idx + window_size + 1)\n",
    "\n",
    "        for context_idx in range(context_start, context_end):\n",
    "            if center_idx != center_word:\n",
    "                context_word = sentence_indices[context_idx]\n",
    "                training_data.append((center_word, context_word))\n",
    "\n",
    "    return np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66bc3698-5cf0-495e-bb10-d9ae9594ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec:\n",
    "    def __init__(self, vocab_size, embed_size=100, learning_rate=0.001):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.learning_rate = learning_rate\n",
    "        # from input to hidden layer\n",
    "        self.W = np.random.uniform(-0.5, 0.5, (vocab_size, embed_size))\n",
    "        # from hidden to output\n",
    "        self.W_prime = np.random.uniform(-0.5, 0.5, (embed_size, vocab_size))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x_exp = np.exp(x - np.max(x))\n",
    "        return x_exp / np.sum(x_exp)\n",
    "\n",
    "    def train(self, training_data, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for center_word, context_word in training_data:\n",
    "                # input to hidden(input isnt written because its one-hot for center word)\n",
    "                h = self.W[center_word]\n",
    "                u = np.dot(h, self.W_prime)\n",
    "                y_pred = self.softmax(u)\n",
    "\n",
    "                y_true = np.zeros(self.vocab_size)\n",
    "                y_true[context_word] = 1\n",
    "\n",
    "                # dloss/dy_pred=y_true*dy_pred/y_pred, dpred/du=dsoftmax(u)\n",
    "                error = y_pred - y_true\n",
    "                # dloss/dw_prime=dloss/dy_pred*dypred/du*du/dw_prime\n",
    "                self.W_prime -= self.learning_rate * np.outer(h, error)\n",
    "                # dloss/dw=dloss/dy_pred*dy_pred/du*du/dh*dh/dw\n",
    "                self.W[center_word] -= self.learning_rate * np.dot(\n",
    "                    error, self.W_prime.T\n",
    "                )\n",
    "\n",
    "                # y_true isnt written because its one-hot for context word\n",
    "                loss -= np.sum(np.log(y_pred[context_word]))\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch :{epoch}, Loss:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1470ef20-c67c-4dca-aa05-ec0b6038c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ملکه زن همسران خانواده شاه مرد سرزمین پهناور زندگی می‌کردند شاه مرد تذکر میداد قدرت اتحاد مرد شاه نهفته قلمرو ملکه زن یادآوری می‌کرد همبستگی زن ملکه مهم داستان مرد شاه زن ملکه می‌آمد حکم می‌گرفت کمک شاه عادل قادر ملکه خردمند زیبا مرد حکمت شاه زن عدالت ملکه راضی می‌رفت شکایت مطرح شاه مرد ملکه زن شاه ملکه نمی‌ترسید شاه زبان میاورد مرد کمک ملکه تأکید زن متحد\n"
     ]
    }
   ],
   "source": [
    "# preprocessing text and filtering stopwords\n",
    "with open(\"persian.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "\n",
    "\n",
    "text = \"ملکه و زن ها در کنار همسران و خانواده خود یعنی شاه و مرد ها در یک سرزمین پهناور زندگی می‌کردند شاه همیشه به مرد ها تذکر میداد که قدرت در اتحاد مرد ها و شاه نهفته است و در این قلمرو ملکه به زن ها یادآوری می‌کرد که همبستگی زن ها و ملکه مهم است و در این داستان هر مرد که نزد شاه یا زن که نزد ملکه می‌آمد از آنها حکم می‌گرفت تا به دیگران کمک کنند شاه عادل و قادر بود و ملکه خردمند و زیبا و هر مرد که از حکمت شاه یا زن که از عدالت ملکه راضی نبود نزد آنها می‌رفت تا شکایت خود را مطرح کند شاه و مرد و ملکه و زن در کنار هم بودند و هیچ کس از شاه یا ملکه نمی‌ترسید شاه همیشه به زبان میاورد که مرد ها باید به یکدیگر کمک کنند و ملکه تأکید داشتند زن ها هم باید متحد باشند\"\n",
    "\n",
    "words = text.split()\n",
    "filtered_text = [word for word in words if word not in stop_words]\n",
    "\n",
    "cleand_text = \" \".join(filtered_text)\n",
    "print(cleand_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f58ab3b4-33a5-4d28-ad0b-6d00a6111354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0, Loss:53.356789408085106\n",
      "Epoch :100, Loss:35.75096563064102\n",
      "Epoch :200, Loss:34.276498804407936\n",
      "Epoch :300, Loss:33.79475420045126\n",
      "Epoch :400, Loss:33.56259002051476\n",
      "Epoch :500, Loss:33.427073104511926\n",
      "Epoch :600, Loss:33.338642132957084\n",
      "Epoch :700, Loss:33.276584117281274\n",
      "Epoch :800, Loss:33.23074489200006\n",
      "Epoch :900, Loss:33.19557221139599\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(cleand_text.split(\" \"))\n",
    "train_data = generate_training_data(vocab, cleand_text.split(\" \"))\n",
    "word2vec_model = Word2vec(vocab.vocab_size)\n",
    "word2vec_model.train(train_data, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9c0a0a2-459f-4860-bd56-1c7eae78aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01514983  0.32530012 -0.14619844  0.20274366  0.3263011   0.09942745\n",
      " -0.41776853  0.3118245  -0.30856868  0.43939278 -0.31279673  0.42955828\n",
      " -0.43436397  0.14516801 -0.09908249 -0.51064728  0.52586765 -0.48288213\n",
      "  0.3252349   0.36155198  0.24463679  0.49802222  0.2449491   0.30149437\n",
      "  0.21963121 -0.26395015 -0.36538522 -0.54492915 -0.2408588  -0.36645249\n",
      "  0.48578376 -0.06634267  0.34094661  0.44863162 -0.43166904 -0.09979197\n",
      "  0.35877761  0.33496308 -0.3725752   0.07430546 -0.31158981  0.37156995\n",
      " -0.02653476  0.01624794 -0.51013922  0.27518936 -0.13202072 -0.36648907\n",
      "  0.52247758 -0.28531512 -0.17128328  0.4558974  -0.52916916  0.21066672\n",
      "  0.20695822 -0.35715205 -0.49090298  0.17846657  0.30772896 -0.25442862\n",
      "  0.36552792 -0.5797372   0.24104936 -0.42488975  0.51584712 -0.18441626\n",
      "  0.3243224  -0.13941809 -0.36663684  0.50584705  0.23580769  0.34287\n",
      "  0.08187537 -0.08127052  0.23602389  0.16436381  0.01783608  0.02897745\n",
      " -0.40595076  0.40958983 -0.22148578  0.46927259 -0.31108872  0.04943226\n",
      " -0.11223114 -0.14253805 -0.01911831 -0.23623112  0.21597005  0.2203835\n",
      " -0.18402907 -0.40247649  0.1328039  -0.14994034  0.23468796  0.45458698\n",
      "  0.4502211   0.43930187 -0.22512738  0.51086314]\n"
     ]
    }
   ],
   "source": [
    "def get_word_embedding(word, vocab, model):\n",
    "    word_idx = vocab.word_to_index(word)\n",
    "    if word_idx != -1:\n",
    "        return model.W[word_idx]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "embedding = get_word_embedding(\"مرد\", vocab, word2vec_model)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5aa44cb-ea99-4ecb-9ed1-96b497b6b979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('کمک', 0.08309825786618957),\n",
       " ('ملکه', 0.06986142746230715),\n",
       " ('شاه', -0.007272829870618061),\n",
       " ('زن', -0.13480019839779123)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the similarity between a word and other words in a sentence, based solely on neighboring\n",
    "def cosine_similarity(word1, word2):\n",
    "    norm_1 = np.linalg.norm(word1)\n",
    "    norm_2 = np.linalg.norm(word2)\n",
    "    return np.dot(word1, word2) / (norm_1 * norm_2)\n",
    "\n",
    "\n",
    "def find_similar_words(word, vocab, model, k=5):\n",
    "    similarity = []\n",
    "    word_idx = vocab.word_to_index(word)\n",
    "\n",
    "    if word_idx == -1:\n",
    "        return f\"{word} does not exist in vocabulary\"\n",
    "\n",
    "    word_vec1 = model.W[word_idx]\n",
    "\n",
    "    for idx in range(vocab.vocab_size):\n",
    "        word_vec2 = model.W[idx]\n",
    "        if idx != word_idx:\n",
    "            embedding = cosine_similarity(word_vec1, word_vec2)\n",
    "            similarity.append((vocab.index_to_word(idx), embedding))\n",
    "            similarity = sorted(similarity, key=lambda x: x[1], reverse=True)\n",
    "    return similarity[:k]\n",
    "\n",
    "\n",
    "similarity = find_similar_words(\"مرد\", vocab, word2vec_model)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f207db36-0260-4e67-875a-b94ef0975a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ملکه', 0.04607401566984035)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Finding a similarity between word_a and word_b that corresponds to the analogy between word_c and analogy(word_a, word_b, word_c)\n",
    "def find_analogy(word_a, word_b, word_c, vocab, model):\n",
    "    vec_a = (\n",
    "        model.W[vocab.word_to_index(word_a)]\n",
    "        if vocab.word_to_index(word_a) != -1\n",
    "        else None\n",
    "    )\n",
    "    vec_b = (\n",
    "        model.W[vocab.word_to_index(word_b)]\n",
    "        if vocab.word_to_index(word_b) != -1\n",
    "        else None\n",
    "    )\n",
    "    vec_c = (\n",
    "        model.W[vocab.word_to_index(word_c)]\n",
    "        if vocab.word_to_index(word_c) != -1\n",
    "        else None\n",
    "    )\n",
    "    if vec_a is None or vec_b is None or vec_c is None:\n",
    "        return f\"one of {word_a} or {word_b} or {word_c} is not in vocabulary\"\n",
    "\n",
    "    target_vec = vec_b - vec_a + vec_c\n",
    "    similarity = []\n",
    "    for idx in range(vocab.vocab_size):\n",
    "        word_vec = model.W[idx]\n",
    "        if (\n",
    "            idx != vocab.word_to_index(word_a)\n",
    "            and idx != vocab.word_to_index(word_b)\n",
    "            and idx != vocab.word_to_index(word_c)\n",
    "        ):\n",
    "            embedding = cosine_similarity(word_vec, target_vec)\n",
    "            similarity.append((vocab.index_to_word(idx), embedding))\n",
    "    similarity = sorted(similarity, key=lambda x: x[1], reverse=True)\n",
    "    return similarity[0]\n",
    "\n",
    "\n",
    "find_analogy(\"زن\", \"شاه\", \"مرد\", vocab, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00884d4-ebaa-428d-a6e2-d5267d886a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(\n",
    "    text,\n",
    "    minimum_length=1,\n",
    "    stopword_removal=True,\n",
    "    stopwords_domain=[],\n",
    "    lower_case=True,\n",
    "    punctuation_removal=True,\n",
    "):\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "\n",
    "    if punctuation_removal:\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if stopword_removal:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.update(stopwords_domain)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [word for word in tokens if len(word) >= minimum_length]\n",
    "\n",
    "    processed_text = \" \".join(tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88393af4-6cbc-4215-8e8d-7f0d25d770bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedWord2Vec:\n",
    "    def __init__(self, preprocessor=None, model=None):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "\n",
    "    def get_query_embedding(self, sentence):\n",
    "        if self.preprocessor:\n",
    "            sentence = self.preprocessor(sentence)\n",
    "\n",
    "        word_vector = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            word_vector.append(model[word])\n",
    "\n",
    "        sentence_vector = np.mean(word_vector, axis=0)\n",
    "\n",
    "        return sentence_vector\n",
    "\n",
    "    def analogy(self, word1, word2, word3):\n",
    "        new_word1 = self.preprocessor(word1)\n",
    "        new_word2 = self.preprocessor(word2)\n",
    "        new_word3 = self.preprocessor(word3)\n",
    "\n",
    "        word_vec1 = model[new_word1]\n",
    "        word_vec2 = model[new_word2]\n",
    "        word_vec3 = model[new_word3]\n",
    "\n",
    "        word_vector = word_vec2 - word_vec1 + word_vec3\n",
    "\n",
    "        vocab_vectors = {word: model[word] for word in list(model.index_to_key)}\n",
    "\n",
    "        possible_results = {\n",
    "            word: vec for word, vec in vocab_vectors.items() if word not in word_vector\n",
    "        }\n",
    "\n",
    "        nearest_word = max(\n",
    "            possible_results,\n",
    "            key=lambda word: np.dot(possible_results[word], word_vector)\n",
    "            / (np.linalg.norm(possible_results[word]) * np.linalg.norm(word_vector)),\n",
    "        )\n",
    "\n",
    "        return nearest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "284e34a2-3b0a-42cc-99bb-2139df8225e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "\n",
    "# # Load a pre-trained skip-gram Word2Vec model (Google News model)\n",
    "# model = api.load('word2vec-google-news-300')\n",
    "model = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f21b4b90-dfa2-4735-9855-ce7e4af8d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = PreTrainedWord2Vec(preprocess_text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbcb8f30-9f6f-499a-99d8-4486ebf04bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: 0.739944338798523, similarity: queens \n",
      "word: 0.7070532441139221, similarity: princess \n",
      "word: 0.6510957479476929, similarity: king \n",
      "word: 0.6383602023124695, similarity: monarch \n",
      "word: 0.6357026100158691, similarity: very_pampered_McElhatton \n",
      "word: 0.6163408160209656, similarity: Queen \n",
      "word: 0.606067955493927, similarity: NYC_anglophiles_aflutter \n",
      "word: 0.5923796892166138, similarity: Queen_Consort \n",
      "word: 0.5908074975013733, similarity: princesses \n",
      "word: 0.5637185573577881, similarity: royal \n",
      "Similarity between man and woman is like similarity between boy and girl\n"
     ]
    }
   ],
   "source": [
    "word = \"queen\"\n",
    "neighboars = w2v.model.most_similar(word)\n",
    "for neighboar in neighboars:\n",
    "    print(f\"word: {neighboar[1]}, similarity: {neighboar[0]} \")\n",
    "\n",
    "word1 = \"man\"\n",
    "word2 = \"woman\"\n",
    "word3 = \"boy\"\n",
    "print(f\"Similarity between {word1} and {word2} is like similarity between {word3} and {w2v.analogy(word1, word2, word3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "306475b7-d2bf-412d-90c5-202caefae207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# DATASET: Positive and Negative Examples\n",
    "positive_examples = [\n",
    "    \"I absolutely loved the movie!\",\n",
    "    \"The food was delicious and the service was excellent.\",\n",
    "    \"This product exceeded my expectations.\",\n",
    "    \"I had a fantastic time at the concert.\",\n",
    "    \"The weather was beautiful during our vacation.\",\n",
    "    \"The customer support team was incredibly helpful.\",\n",
    "    \"I would definitely recommend this restaurant.\",\n",
    "    \"The book was a joy to read from start to finish.\",\n",
    "    \"I'm very happy with my purchase.\",\n",
    "    \"The experience was truly unforgettable.\"\n",
    "]\n",
    "\n",
    "negative_examples = [\n",
    "    \"The movie was terrible and a complete waste of time.\",\n",
    "    \"The food was cold and the service was slow.\",\n",
    "    \"I was very disappointed with the quality of this product.\",\n",
    "    \"The concert was boring and not worth the money.\",\n",
    "    \"The weather ruined our entire trip.\",\n",
    "    \"The customer support team was unhelpful and rude.\",\n",
    "    \"I will never visit this restaurant again.\",\n",
    "    \"The book was poorly written and hard to follow.\",\n",
    "    \"I'm extremely dissatisfied with my purchase.\",\n",
    "    \"The experience was awful and I regret it.\"\n",
    "]\n",
    "\n",
    "positive_labels=[1]*len(positive_examples)\n",
    "positive_sentence=list(zip(positive_examples,positive_labels))\n",
    "negative_labels=[0]*len(negative_examples)\n",
    "negative_sentence=list(zip(negative_examples,negative_labels))\n",
    "dataset=positive_sentence+negative_sentence\n",
    "\n",
    "random.shuffle(dataset)\n",
    "train_data,val_data=train_test_split(dataset,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7303c3e0-34f6-41a5-afab-e4729f9e519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Dataset class, the __len__ and __getitem__ methods are abstract and need to be implemented by subclasses.\n",
    "class SentimentDataset(Dataset): \n",
    "    def __init__(self, data, word2vec_model):\n",
    "        self.data = data\n",
    "        self.word2vec_model = word2vec_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_embedding(self, sentence):\n",
    "        embedding = self.word2vec_model.get_query_embedding(sentence)\n",
    "        return torch.tensor(embedding).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.data[idx]\n",
    "        embedding = self.get_embedding(sentence)\n",
    "        return embedding, torch.tensor(label, dtype=torch.float)\n",
    "        \n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.stack_layers = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stack_layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b50d8e2-ff5d-465b-9b2a-e782d5fbdbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "train_dataset = SentimentDataset(train_data, w2v)\n",
    "val_dataset = SentimentDataset(val_data, w2v)\n",
    "\n",
    "# When a DataLoader is created from an object of the SentimentDataset class,some of its functions (like __len__ and __getitem__)are called automatically\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "nn_model = SentimentClassifier(embedding_dim)\n",
    "nn_optimizer = optim.Adam(nn_model.parameters(), lr=1e-4)\n",
    "nn_loss = nn.BCELoss()\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "def train(dataloader, model, optimizer, loss_nn):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch_idx, (data, label) in enumerate(dataloader):\n",
    "        y_pred = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_nn(y_pred.squeeze(), label)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_nn):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(dataloader):\n",
    "            y_pred = model(data)\n",
    "            running_loss += loss_nn(y_pred.squeeze(), label).item()\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2228b789-cc67-434c-bb34-46472bb1488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 2.0355| Val loss: 0.8368: 100%|██████████████████████████████████████████| 100/100 [00:04<00:00, 23.41it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for t in (pbar := trange(num_epochs)):\n",
    "    loss = train(train_loader, nn_model, nn_optimizer, nn_loss)\n",
    "    val_loss = test(val_loader, nn_model, nn_loss)\n",
    "    pbar.set_description(f\"Train loss: {loss:.4f}| Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "844c7d47-d47b-4eba-b434-e89be8cfa71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_vector = w2v.get_query_embedding(query_sentence)\n",
    "        test_vector = torch.tensor(test_vector)\n",
    "        test_pred = model(test_vector)\n",
    "        sentiment = \"positive\" if test_pred.item() >= 0.5 else \"negative\"\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7e29f01-d81a-4e4d-9bc6-4cd110ab8ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sentence = \"The movie was terrible and a i hated it\"\n",
    "sentiment=predict_sentiment(nn_model, query_sentence)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82a8d9d5-a7de-4da2-9bd7-dead8e70c7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sentence = \"The movie was amazing and i loved it\"\n",
    "sentiment=predict_sentiment(nn_model, query_sentence)\n",
    "sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
